{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"},{"sourceId":205106,"sourceType":"modelInstanceVersion","modelInstanceId":22009,"modelId":3533}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import PowerTransformer\nimport xgboost as xgb\nfrom scipy.stats import boxcox_normmax\nfrom scipy.optimize import OptimizeWarning\nfrom scipy.special import boxcox1p\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n## DATA LOADING ##\ntrain = pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv')\ntest = pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')\n\n## DATA PREPARATION ##\n# Save IDs and target\ntrain_ids = train['Id']\ntest_ids = test['Id']\ny_train = np.log1p(train['SalePrice'])  # Log transform target\n\n# Prepare features\nX_train = train.drop(['Id', 'SalePrice'], axis=1)\nX_test = test.drop('Id', axis=1)\n\n## MISSING VALUE HANDLING ##\ndef handle_missing_values(df):\n    # Categorical features\n    cat_missing_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', \n                       'FireplaceQu', 'GarageType', 'GarageFinish', \n                       'GarageQual', 'GarageCond', 'BsmtQual', \n                       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n                       'BsmtFinType2', 'MasVnrType']\n    \n    for col in cat_missing_none:\n        if col in df.columns:\n            df[col] = df[col].fillna('None')\n    \n    # Numerical features\n    num_missing_zero = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n                       'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', \n                       'BsmtHalfBath', 'GarageArea', 'GarageCars']\n    \n    for col in num_missing_zero:\n        if col in df.columns:\n            df[col] = df[col].fillna(0)\n    \n    # Special cases\n    if 'LotFrontage' in df.columns:\n        df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(\n            lambda x: x.fillna(x.median()))\n    \n    if 'GarageYrBlt' in df.columns:\n        df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['YearBuilt'])\n    \n    # Few missing values\n    few_missing_cat = ['MSZoning', 'Electrical', 'Functional', \n                      'KitchenQual', 'Exterior1st', 'Exterior2nd', \n                      'SaleType']\n    \n    for col in few_missing_cat:\n        if col in df.columns:\n            df[col] = df[col].fillna(df[col].mode()[0])\n    \n    if 'Utilities' in df.columns:\n        df.drop('Utilities', axis=1, inplace=True)\n    \n    return df\n\nX_train = handle_missing_values(X_train)\nX_test = handle_missing_values(X_test)\n\n## FEATURE ENGINEERING ##\ndef create_features(df):\n    # Total square footage\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    \n    # Total bathrooms\n    df['TotalBath'] = df['FullBath'] + (0.5 * df['HalfBath']) + \\\n                     df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath'])\n    \n    # Age features\n    df['Age'] = df['YrSold'] - df['YearBuilt']\n    df['RemodAge'] = df['YrSold'] - df['YearRemodAdd']\n    \n    # Boolean features\n    df['HasBasement'] = (df['TotalBsmtSF'] > 0).astype(int)\n    df['HasPool'] = (df['PoolArea'] > 0).astype(int)\n    \n    # Ratios\n    df['LivLotRatio'] = df['GrLivArea'] / df['LotArea']\n    df['Spaciousness'] = (df['1stFlrSF'] + df['2ndFlrSF']) / df['TotRmsAbvGrd']\n    \n    return df\n\nX_train = create_features(X_train)\nX_test = create_features(X_test)\n\n## ROBUST TRANSFORMATION FUNCTION ##\ndef safe_feature_transform(df, features, method='boxcox'):\n    \"\"\"\n    Robust feature transformation with comprehensive error handling\n    Supports both Box-Cox and Yeo-Johnson methods\n    \n    Parameters:\n    - df: DataFrame containing features\n    - features: List of features to transform\n    - method: 'boxcox' or 'yeojohnson'\n    \n    Returns:\n    - Transformed DataFrame\n    - Dictionary of lambdas/transforms used\n    \"\"\"\n    transform_info = {}\n    \n    if method == 'yeojohnson':\n        pt = PowerTransformer(method='yeo-johnson', standardize=False)\n        \n        for feature in features:\n            try:\n                # Reshape data for sklearn\n                data = df[feature].values.reshape(-1, 1)\n                \n                # Fit and transform\n                transformed = pt.fit_transform(data)\n                df[feature] = transformed.flatten()\n                transform_info[feature] = 'yeojohnson'\n            except Exception as e:\n                print(f\"Yeo-Johnson failed for {feature}: {str(e)}\")\n                transform_info[feature] = 'failed'\n        \n        return df, transform_info\n    \n    # Box-Cox implementation\n    for feature in features:\n        try:\n            # Skip if feature has no variation\n            if df[feature].nunique() <= 1:\n                print(f\"Skipping {feature} - no variation\")\n                transform_info[feature] = 'skipped'\n                continue\n                \n            # Handle zeros/negatives by shifting\n            shift = 1 - df[feature].min() if df[feature].min() <= 0 else 0\n            data = df[feature] + shift\n            \n            # Find optimal lambda with error handling\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"error\", OptimizeWarning)\n                try:\n                    optimal_lambda = boxcox_normmax(data, brack=(-5, 5))\n                    \n                    # Apply transformation\n                    df[feature] = boxcox1p(df[feature], optimal_lambda)\n                    transform_info[feature] = optimal_lambda\n                    \n                except (ValueError, Warning) as e:\n                    print(f\"Box-Cox optimization warning for {feature}: {str(e)}\")\n                    # Fallback to log transform\n                    df[feature] = np.log1p(df[feature])\n                    transform_info[feature] = 'log'\n                    \n        except Exception as e:\n            print(f\"Transform failed for {feature}: {str(e)}\")\n            transform_info[feature] = 'failed'\n    \n    return df, transform_info\n## HOW TO USE ##\n# Identify skewed features\nnumeric_feats = X_train.select_dtypes(include=[np.number]).columns\nskewness = X_train[numeric_feats].skew().sort_values(ascending=False)\nhigh_skew_features = skewness[abs(skewness) > 0.75].index\n\n# Option 1: Use Yeo-Johnson (recommended - more robust)\nX_train, train_transforms = safe_feature_transform(X_train.copy(), high_skew_features, method='yeojohnson')\nX_test, _ = safe_feature_transform(X_test.copy(), high_skew_features, method='yeojohnson')\n\n## ENCODE CATEGORICAL VARIABLES ##\n# Label encode ordinal categoricals\nordinal_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n               'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', \n               'GarageCond', 'PoolQC']\n\nfor col in ordinal_cols:\n    if col in X_train.columns:\n        le = LabelEncoder()\n        le.fit(pd.concat([X_train[col], X_test[col]], axis=0))\n        X_train[col] = le.transform(X_train[col])\n        X_test[col] = le.transform(X_test[col])\n\n# One-hot encode remaining categoricals\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\n\n# Align train and test sets\nX_train, X_test = X_train.align(X_test, join='inner', axis=1)\n\n## MODEL BUILDING ##\n# Initialize XGBoost model\nxgb_model = xgb.XGBRegressor(\n    n_estimators=1000,\n    learning_rate=0.01,\n    max_depth=4,\n    subsample=0.9,\n    colsample_bytree=0.4,\n    random_state=42\n)\n\n# Cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(xgb_model, X_train, y_train, \n                       cv=kf, \n                       scoring='neg_mean_squared_error')\n\n# Calculate RMSE\nrmse_scores = np.sqrt(-scores)\nprint(f\"CV RMSE: {rmse_scores.mean():.4f} ± {rmse_scores.std():.4f}\")\n\n## HYPERPARAMETER TUNING ##\nparam_grid = {\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.3, 0.4, 0.5]\n}\n\ngrid_search = GridSearchCV(estimator=xgb_model, \n                          param_grid=param_grid,\n                          cv=5,\n                          scoring='neg_mean_squared_error',\n                          n_jobs=-1)\n\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\n\n## FINAL MODEL ##\nfinal_model = xgb.XGBRegressor(**best_params)\nfinal_model.fit(X_train, y_train)\n\n## PREDICTION AND SUBMISSION ##\ntest_preds = final_model.predict(X_test)\nfinal_preds = np.expm1(test_preds)  # Convert from log scale back to dollars\n\nsubmission = pd.DataFrame({\n    'Id': test_ids,\n    'SalePrice': final_preds\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"_uuid":"d7105537-cb99-4a86-a9f2-82114a68fc3c","_cell_guid":"381f795c-fde4-4151-b933-996976746677","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-25T23:12:41.876097Z","iopub.execute_input":"2025-03-25T23:12:41.876441Z","iopub.status.idle":"2025-03-25T23:16:53.404167Z","shell.execute_reply.started":"2025-03-25T23:12:41.876418Z","shell.execute_reply":"2025-03-25T23:16:53.403346Z"}},"outputs":[{"name":"stdout","text":"CV RMSE: 0.1254 ± 0.0189\n","output_type":"stream"}],"execution_count":4}]}